{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objs as goa\n",
    "import matplotlib as mpl\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from fancy_einsum import einsum\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n",
    "import examples_setup as utils\n",
    "import copy\n",
    "mpl.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "fontsize = 20\n",
    "mpl.rcParams['font.size'] = fontsize\n",
    "mpl.rcParams['xtick.labelsize'] = fontsize\n",
    "mpl.rcParams['ytick.labelsize'] = fontsize\n",
    "mpl.rcParams['legend.fontsize'] = fontsize\n",
    "mpl.rcParams['axes.titlesize'] = fontsize\n",
    "mpl.rcParams['axes.labelsize'] = fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 12\n",
    "utils.set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 40\n",
    "k = 1\n",
    "n = 2\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = F.relu\n",
    "tied = False\n",
    "final_bias = True\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "\n",
    "\n",
    "epochs = 150000\n",
    "all_epochs = 250000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 5\n",
    "initial_lr = 0.001\n",
    "warmup_frac = 0.05\n",
    "final_lr = 2\n",
    "decay_factor=(final_lr/max_lr)**(1/(all_epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(all_epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "model = utils.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(v1, v2, degrees = False):\n",
    "    dot_product = torch.dot(v1, v2)\n",
    "    v1_norm = torch.linalg.norm(v1)\n",
    "    v2_norm = torch.linalg.norm(v2)\n",
    "    cos_theta = dot_product / (v1_norm * v2_norm)\n",
    "\n",
    "    # Convert to degrees\n",
    "    theta = torch.acos(cos_theta)\n",
    "    if degrees:\n",
    "        theta *= (180 / torch.pi)\n",
    "    return theta\n",
    "\n",
    "class ControllableNet(utils.Net):\n",
    "    def __init__(self, net: utils.Net, free_rows = [11]):\n",
    "        super().__init__(net.input_dim, net.hidden_dim, tied = False, final_bias = True, hidden_bias = False, nonlinearity = F.relu, unit_weights=False, learnable_scale_factor = False, standard_magnitude = False, initial_scale_factor = 1.0, initial_embed = None, initial_bias = None)\n",
    "        self.free_rows = free_rows\n",
    "        # Define the input layer (embedding)\n",
    "        self.controlled_mask = torch.ones(len(net.embedding.weight.data.T), dtype=torch.bool)\n",
    "        self.controlled_mask[free_rows] = False\n",
    "        \n",
    "        self.embedding_free = nn.Linear(len(free_rows), self.hidden_dim, bias=False)\n",
    "        self.embedding_free.weight.data = net.embedding.weight.data[:,~self.controlled_mask]\n",
    "\n",
    "        self.embedding_controlled = nn.Linear(self.input_dim - len(free_rows), self.hidden_dim, bias=False)\n",
    "        self.embedding_controlled.weight.data = net.embedding.weight.data[:,self.controlled_mask]\n",
    "        for param in self.embedding_controlled.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.unembedding_free = nn.Linear(self.hidden_dim, len(free_rows), bias=True)\n",
    "        self.unembedding_free.weight.data = net.unembedding.weight.data[~self.controlled_mask]\n",
    "        self.unembedding_free.bias.data = net.unembedding.bias.data[~self.controlled_mask]\n",
    "\n",
    "        self.unembedding_controlled = nn.Linear(self.hidden_dim, self.input_dim - len(free_rows), bias=True)\n",
    "        self.unembedding_controlled.weight.data = net.unembedding.weight.data[self.controlled_mask]\n",
    "        self.unembedding_controlled.bias.data = net.unembedding.bias.data[~self.controlled_mask]\n",
    "\n",
    "\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_dim, bias=False)\n",
    "        self.unembedding = nn.Linear(self.hidden_dim, self.input_dim, bias=False)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.unembedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.embedding.weight.data = net.embedding.weight.data\n",
    "        self.unembedding.weight.data = net.unembedding.weight.data\n",
    "        self.unembedding.bias.data = net.unembedding.bias.data\n",
    "\n",
    "        for param in self.unembedding_controlled.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def angle_between(self, rows = [1,18], in_embedding = True):\n",
    "        matrix = self.embedding.weight.data.T if in_embedding else self.unembedding.weight.data\n",
    "        vec1, vec2 = matrix[rows[0]], matrix[rows[1]]\n",
    "        return calculate_angle(vec1,vec2)\n",
    "    \n",
    "    def reduce_angle(self, rows = [1,18], in_embedding = True, reduction = 0.01):\n",
    "        assert all([row not in self.free_rows for row in rows])\n",
    "        matrix = self.embedding.weight.data.T if in_embedding else self.unembedding.weight.data\n",
    "        vec1, vec2 = matrix[rows[0]], matrix[rows[1]]\n",
    "        magnitude1, magnitude2 = torch.linalg.norm(vec1), torch.linalg.norm(vec2)\n",
    "        theta = calculate_angle(vec1, vec2)\n",
    "\n",
    "        average = 1/2 * (vec1/magnitude1 + vec2/magnitude2)\n",
    "        phi = torch.atan2(average[1],average[0])\n",
    "\n",
    "        new_v1 = magnitude1 * torch.tensor([torch.cos(phi - theta/2 + reduction/2), torch.sin(phi - theta/2 + reduction/2)])\n",
    "        new_v2 = magnitude2 * torch.tensor([torch.cos(phi + theta/2 - reduction/2), torch.sin(phi + theta/2 - reduction/2)])\n",
    "        assert calculate_angle(new_v1, new_v2) == theta - reduction, print(theta, calculate_angle(new_v1, new_v2))\n",
    "        matrix[rows[0]] = new_v1\n",
    "        matrix[rows[1]] = new_v2\n",
    "        if in_embedding:\n",
    "            self.embedding.weight.data = matrix.T\n",
    "            self.embedding_controlled.weight.data = self.embedding.weight.data[:,self.controlled_mask]\n",
    "        if not in_embedding:\n",
    "            self.embedding.weight.data = matrix.T\n",
    "            self.embedding_controlled.weight.data = self.embedding.weight.data[:,self.controlled_mask]\n",
    "    \n",
    "    def embedding_forward(self,x):\n",
    "        matrix = torch.zeros(self.input_dim, self.hidden_dim)\n",
    "        matrix[self.controlled_mask] = self.embedding_controlled.weight\n",
    "        matrix[~self.controlled_mask] = self.embedding_free.weight\n",
    "        return matrix@x\n",
    "\n",
    "    def unembedding_forward(self,x):\n",
    "        matrix = torch.zeros(self.hidden_dim, self.input_dim)\n",
    "        matrix[self.controlled_mask] = self.unembedding_controlled.weight\n",
    "        matrix[~self.controlled_mask] = self.unembedding_free.weight\n",
    "        vector = torch.zeros(self.input_dim)\n",
    "        vector[self.controlled_mask] = self.unembedding_controlled.bias\n",
    "        vector[~self.controlled_mask] = self.unembedding_free.bias\n",
    "        return matrix@x + vector\n",
    "\n",
    "    def forward(self, x, hooked = False):\n",
    "        if hooked:\n",
    "            activations = {}\n",
    "            activations['res_pre'] = self.embedding_forward(x)\n",
    "            activations['unembed_pre'] = self.unembedding_forward(activations['res_pre'])\n",
    "            activations['output'] = self.scale_factor * self.nonlinearity(activations['unembed_pre'])\n",
    "            return activations['output'], activations\n",
    "        else:\n",
    "            x = self.embedding_forward(x)\n",
    "            x = self.unembedding_forward(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            return self.scale_factor * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_angle(model, rows = [1,18], in_embedding = True, reduction = 0.01):\n",
    "    matrix = model.embedding.weight.data.T if in_embedding else model.unembedding.weight.data\n",
    "    vec1, vec2 = matrix[rows[0]], matrix[rows[1]]\n",
    "    magnitude1, magnitude2 = torch.linalg.norm(vec1), torch.linalg.norm(vec2)\n",
    "    theta = calculate_angle(vec1, vec2)\n",
    "\n",
    "    average = 1/2 * (vec1/magnitude1 + vec2/magnitude2)\n",
    "    phi = torch.atan2(average[1],average[0])\n",
    "\n",
    "    new_v1 = magnitude1 * torch.tensor([torch.cos(phi - theta/2 + reduction/2), torch.sin(phi - theta/2 + reduction/2)])\n",
    "    new_v2 = magnitude2 * torch.tensor([torch.cos(phi + theta/2 - reduction/2), torch.sin(phi + theta/2 - reduction/2)])\n",
    "    assert calculate_angle(new_v1, new_v2) == theta - reduction, print(theta, calculate_angle(new_v1, new_v2))\n",
    "    matrix[rows[0]] = new_v1\n",
    "    matrix[rows[1]] = new_v2\n",
    "    if in_embedding:\n",
    "        model.embedding.weight.data = matrix.T\n",
    "    if not in_embedding:\n",
    "        model.unembedding.weight.data = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_schedule(model, rows, epochs, end = 0, in_embedding = False):\n",
    "    matrix = model.embedding.weight.data.T if in_embedding else model.unembedding.weight.data\n",
    "    vec1, vec2 = matrix[rows[0]], matrix[rows[1]]\n",
    "    magnitude1, magnitude2 = torch.linalg.norm(vec1), torch.linalg.norm(vec2)\n",
    "    theta = calculate_angle(vec1, vec2).item()\n",
    "    average = 1/2 * (vec1/magnitude1 + vec2/magnitude2)\n",
    "    phi = torch.atan2(average[1],average[0]).item()\n",
    "    reductions = np.linspace(theta, theta * end, epochs)\n",
    "\n",
    "    new_v1 = magnitude1 * torch.tensor(np.array([np.cos(phi - theta/2 + reductions/2), np.sin(phi - theta/2 + reductions/2)]))\n",
    "    new_v2 = magnitude2 * torch.tensor(np.array([np.cos(phi + theta/2 - reductions/2), np.sin(phi + theta/2 - reductions/2)]))\n",
    "    return [new_v1.T, new_v2.T]\n",
    "\n",
    "def train_controlled(model: utils.Net,\n",
    "                   loader, criterion, optimizer, epochs, logging_loss, store_rate,\n",
    "                   rows, end,\n",
    "                   scheduler = None):\n",
    "    weights_history = {k:[v.detach().numpy().copy()] for k,v in dict(model.named_parameters()).items()}  # Store the weights here\n",
    "    model_history = {} #store model here\n",
    "    losses = []\n",
    "    chosen_weights = weights_schedule(model,rows,epochs,end)\n",
    "    biases = model.unembedding.bias.data\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            # if in_embedding:\n",
    "            #     for row in rows:\n",
    "            #         model.embedding.weight.data[:,row] = chosen_weights[0][epoch]\n",
    "            # else:\n",
    "            for row in rows:\n",
    "                model.unembedding.weight.data[row] = chosen_weights[0][epoch]\n",
    "                model.unembedding.bias.data[row] = biases[row]\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        if logging_loss:\n",
    "            losses.append(avg_loss)\n",
    "            if plot_rate > 0:\n",
    "                if (epoch + 1) % plot_rate == 0:\n",
    "                    plt.figure(figsize=(5,5))\n",
    "                    plt.plot(losses)\n",
    "                    plt.show()\n",
    "        if (epoch + 1) % store_rate == 0:\n",
    "            for k,v in dict(model.named_parameters()).items():\n",
    "                weights_history[k].append(v.detach().numpy().copy())\n",
    "            model_history[epoch] = copy.deepcopy(model)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    return losses, weights_history, model_history  # Return the weights history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 12\n",
    "utils.set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "MSE = True\n",
    "\n",
    "# Configure the hyperparameters\n",
    "free_rows = [11]\n",
    "\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "logging_loss = True\n",
    "angle = calculate_angle(model.embedding.weight.data.T[1], model.embedding.weight.data.T[18])\n",
    "reductions = [angle/epochs] * epochs\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 1\n",
    "initial_lr = 1\n",
    "warmup_frac = 0.05\n",
    "final_lr = 1\n",
    "decay_factor=(final_lr/max_lr)**(1/(all_epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(all_epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "\n",
    "# Assume `original_model` is your existing PyTorch model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Create a new model object\n",
    "new_model = utils.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Load the copied state dict into the new model object\n",
    "new_model.load_state_dict(state_dict)\n",
    "\n",
    "#Train\n",
    "new_losses, new_weights_history, model_history = train_controlled(new_model, loader,criterion,optimizer,epochs,logging_loss,store_rate,rows=[1,18],scheduler=scheduler, end = 0.9)\n",
    "\n",
    "# Train the model\n",
    "# new_losses, new_weights_history, new_model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(new_weights_history, store_rate=store_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
