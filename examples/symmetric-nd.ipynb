{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objs as goa\n",
    "import matplotlib as mpl\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from fancy_einsum import einsum\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n",
    "import examples_setup as utils\n",
    "mpl.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "fontsize = 20\n",
    "mpl.rcParams['font.size'] = fontsize\n",
    "mpl.rcParams['xtick.labelsize'] = fontsize\n",
    "mpl.rcParams['ytick.labelsize'] = fontsize\n",
    "mpl.rcParams['legend.fontsize'] = fontsize\n",
    "mpl.rcParams['axes.titlesize'] = fontsize\n",
    "mpl.rcParams['axes.labelsize'] = fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 100\n",
    "utils.set_seed(chosen_seed)\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "# Configure the hyperparameters\n",
    "f = 40\n",
    "k = 1\n",
    "n = 2\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = F.relu\n",
    "tied = True\n",
    "final_bias = False\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "epochs = 10000\n",
    "logging_loss = True\n",
    "#Scheduler params\n",
    "max_lr = 2\n",
    "initial_lr = 0.02\n",
    "warmup_frac = 0.05\n",
    "final_lr = 0.5\n",
    "decay_factor=(final_lr/max_lr)**(1/(epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(epochs * warmup_frac)\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "model = utils.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 101\n",
    "new_run = True\n",
    "utils.set_seed(chosen_seed)\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "# Configure the hyperparameters\n",
    "f = 250\n",
    "k = 1\n",
    "n = 9\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = F.relu\n",
    "tied = True\n",
    "final_bias = False\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "epochs = 1000000\n",
    "logging_loss = True\n",
    "#Scheduler params\n",
    "max_lr = 8\n",
    "initial_lr = 0.02\n",
    "warmup_frac = 0.05\n",
    "final_lr = 1\n",
    "decay_factor=(final_lr/max_lr)**(1/(epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(epochs * warmup_frac)\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "if new_run:\n",
    "    model = utils.Net(f, n,\n",
    "                tied = tied,\n",
    "                final_bias = final_bias,\n",
    "                hidden_bias = hidden_bias,\n",
    "                nonlinearity=nonlinearity,\n",
    "                unit_weights=unit_weights,\n",
    "                learnable_scale_factor=learnable_scale_factor,\n",
    "                standard_magnitude=standard_magnitude,\n",
    "                initial_scale_factor = initial_scale_factor,\n",
    "                initial_embed = initial_embed,\n",
    "                initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "# Train the model\n",
    "if new_run:\n",
    "    losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)\n",
    "else:\n",
    "    new_losses, new_weights_history, new_model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)\n",
    "    losses += new_losses\n",
    "    weights_history += new_weights_history\n",
    "    model_history += new_model_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, directions = utils.group_vectors(model.unembedding.weight.data.detach().numpy(), 0.001)\n",
    "print(len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.detach().pow(2).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = {}\n",
    "models = {}\n",
    "for n in range(2,11):\n",
    "    print(f'current n: {n}')\n",
    "    n_groups[n] = []\n",
    "    models[n] = []\n",
    "    for chosen_seed in range(1,6):\n",
    "        print(f'current seed: {chosen_seed}')\n",
    "        utils.set_seed(chosen_seed)\n",
    "        #Checking for errors\n",
    "        lr_print_rate = 0\n",
    "        # Configure the hyperparameters\n",
    "        f = 100\n",
    "        k = 1\n",
    "        MSE = True #else Crossentropy\n",
    "        nonlinearity = F.relu\n",
    "        tied = True\n",
    "        final_bias = False\n",
    "        hidden_bias = False\n",
    "        unit_weights = False\n",
    "        learnable_scale_factor = False\n",
    "        initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "        standard_magnitude = False\n",
    "        initial_embed = None\n",
    "        initial_bias = None\n",
    "        epochs = 40000\n",
    "        logging_loss = True\n",
    "        #Scheduler params\n",
    "        max_lr = 3\n",
    "        initial_lr = 0.02\n",
    "        warmup_frac = 0.05\n",
    "        final_lr = 1\n",
    "        decay_factor=(final_lr/max_lr)**(1/(epochs * (1-warmup_frac)))\n",
    "        warmup_steps = int(epochs * warmup_frac)\n",
    "        store_rate = epochs//100\n",
    "        plot_rate=0 #epochs/5\n",
    "        # Instantiate synthetic dataset\n",
    "        dataset = utils.SyntheticKHot(f,k)\n",
    "        batch_size = len(dataset) #Full batch gradient descent\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "        #Define the Loss function\n",
    "        criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "        # Instantiate the model\n",
    "        # initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "        # initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "        model = utils.Net(f, n,\n",
    "                    tied = tied,\n",
    "                    final_bias = final_bias,\n",
    "                    hidden_bias = hidden_bias,\n",
    "                    nonlinearity=nonlinearity,\n",
    "                    unit_weights=unit_weights,\n",
    "                    learnable_scale_factor=learnable_scale_factor,\n",
    "                    standard_magnitude=standard_magnitude,\n",
    "                    initial_scale_factor = initial_scale_factor,\n",
    "                    initial_embed = initial_embed,\n",
    "                    initial_bias = initial_bias)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "        #Define a learning rate schedule\n",
    "        scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "        # Train the model\n",
    "        losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)\n",
    "        groups, directions = utils.group_vectors(model.unembedding.weight.data.detach().numpy(), 0.001)\n",
    "        n_groups[n].append(len(groups))\n",
    "        models[n].append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups3 = {}\n",
    "for n in models.keys():\n",
    "        n_groups2[n] = []\n",
    "        for seed in range(len(models[n])):\n",
    "                groups, directions = utils.group_vectors(models[n][seed].unembedding.weight.data.detach().numpy(), 0.1)\n",
    "                n_groups2[n].append(len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare your data for the linear regression model\n",
    "x = np.array(list(n_groups2.keys()))\n",
    "y = np.array([np.mean(val) for val in n_groups2.values()])\n",
    "\n",
    "# Reshape your data since it has a single feature\n",
    "x = x.reshape((-1, 1))\n",
    "\n",
    "# Apply linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# The line of best fit is represented as: y = mx + c\n",
    "m = model.coef_[0]\n",
    "c = model.intercept_\n",
    "\n",
    "print(f\"The line of best fit is: y = {m}x + {c}\")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "x = np.array(list(n_groups2.keys())).reshape((-1, 1))\n",
    "y = np.array([np.mean(val) for val in n_groups2.values()])\n",
    "\n",
    "# Create a model with polynomial features\n",
    "model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "model.fit(x, y)\n",
    "\n",
    "# The coefficients of the quadratic model (quadratic term, linear term, and intercept)\n",
    "coeff = model.named_steps['linearregression'].coef_\n",
    "intercept = model.named_steps['linearregression'].intercept_\n",
    "\n",
    "print(f'The equation of best fit is: y = {coeff[2]}x^2 + {coeff[1]}x + {intercept}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1,10,500)\n",
    "for n in range(2,11):\n",
    "    plt.scatter([n]*5, n_groups2[n], color='blue')\n",
    "plt.plot(xs, xs * 7.52-12.853)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Number of Features Encoded')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups2 = {}\n",
    "for n in models.keys():\n",
    "        n_groups2[n] = []\n",
    "        for seed in range(len(models[n])):\n",
    "                groups, directions = utils.group_vectors(models[n][seed].unembedding.weight.data.detach().numpy(), 0.01)\n",
    "                n_groups2[n].append(len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "n = 5\n",
    "weights['embedding.weight'] = [m.embedding.weight for m in models[n]]\n",
    "weights['unembedding.weight'] = [m.unembedding.weight for m in models[n]]\n",
    "weights['unembedding.bias'] = [m.unembedding.bias for m in models[n]]\n",
    "\n",
    "utils.plot_weights_interactive(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
