{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Model Without Tied Embedding and Unembedding\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib as mpl\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from fancy_einsum import einsum\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "fontsize = 20\n",
    "mpl.rcParams['font.size'] = fontsize\n",
    "mpl.rcParams['xtick.labelsize'] = fontsize\n",
    "mpl.rcParams['ytick.labelsize'] = fontsize\n",
    "mpl.rcParams['legend.fontsize'] = fontsize\n",
    "mpl.rcParams['axes.titlesize'] = fontsize\n",
    "mpl.rcParams['axes.labelsize'] = fontsize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_samples, f):\n",
    "        self.num_samples = num_samples\n",
    "        self.f = f\n",
    "        self.data = self.generate_data()\n",
    "        \n",
    "    def generate_data(self):\n",
    "        data = torch.zeros((self.num_samples, self.f))\n",
    "        for i in range(self.num_samples):\n",
    "            index = torch.randint(0, self.f, (1,))\n",
    "            data[i, index] = torch.rand(1)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SyntheticNormalised(Dataset):\n",
    "    #Creates a dataset with f 1-hot vectors as the dataset.\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.data = self.generate_data()\n",
    "        \n",
    "    def generate_data(self):\n",
    "        return torch.eye(self.f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.f\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SyntheticKHot(Dataset):\n",
    "    def __init__(self, f, k):\n",
    "        self.f = f\n",
    "        self.k = k\n",
    "        self.data = []\n",
    "\n",
    "        # Create all possible combinations of f choose k\n",
    "        for indices in combinations(range(f), k):\n",
    "            vec = torch.zeros(f)\n",
    "            vec[list(indices)] = 1\n",
    "            self.data.append(vec)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.data[idx]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, tied = True, final_bias = False, nonlinearity = F.relu, unit_weights=False, with_scale_factor = False, standard_magnitude = False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.tied = tied\n",
    "        self.final_bias = final_bias\n",
    "        self.unit_weights = unit_weights\n",
    "        self.with_scale_factor = with_scale_factor\n",
    "        self.standard_magnitude = standard_magnitude\n",
    "\n",
    "\n",
    "        # Define the input layer (embedding)\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        # Define the output layer (unembedding)\n",
    "        self.unembedding = nn.Linear(self.hidden_dim, self.input_dim, bias=final_bias)\n",
    "\n",
    "        if self.standard_magnitude:\n",
    "            # Normalize the weight to have unit norm for each row\n",
    "            avg_norm = torch.norm(self.embedding.weight.data, p=2, dim = 0).mean()\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0) * avg_norm\n",
    "        if self.unit_weights:\n",
    "            # Normalize the weight to have unit norm for each row\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "        # Tie the weights\n",
    "        if tied:\n",
    "            self.unembedding.weight = torch.nn.Parameter(self.embedding.weight.transpose(0, 1))\n",
    "\n",
    "        if self.with_scale_factor:\n",
    "            self.scale_factor = nn.Parameter(torch.tensor(1.0))\n",
    "        else:\n",
    "            self.scale_factor = 1.0\n",
    "\n",
    "    def forward(self, x, hooked = False):\n",
    "        if self.unit_weights:\n",
    "            # Normalize the weight to have unit norm for each row\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "        if self.standard_magnitude:\n",
    "            avg_norm = torch.norm(self.embedding.weight.data, p=2, dim = 0).mean()\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "            self.embedding.weight.data = self.embedding.weight.data * avg_norm\n",
    "        if self.tied:\n",
    "            self.unembedding.weight.data = self.embedding.weight.data.transpose(0, 1)\n",
    "        if hooked:\n",
    "            activations = {}\n",
    "            activations['res_pre'] = self.embedding(x)\n",
    "            activations['unembed_pre'] = self.unembedding(activations['res_pre'])\n",
    "            activations['output'] = self.scale_factor * self.nonlinearity(activations['unembed_pre'])\n",
    "            return activations['output'], activations\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = self.unembedding(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            return self.scale_factor * x\n",
    "\n",
    "class ResNet(Net):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 mlp_dim,\n",
    "                 tied = True,\n",
    "                 mlp_tied = True,\n",
    "                 mlp_bias = False,\n",
    "                 final_bias = False,\n",
    "                 nonlinearity = F.relu,\n",
    "                 n_mlps = 1):\n",
    "        super().__init__(input_dim, hidden_dim, tied, final_bias, nonlinearity)\n",
    "\n",
    "        mlp_ins = []\n",
    "        mlp_outs = []\n",
    "        for i in range(n_mlps):\n",
    "            mlp_in = nn.Linear(hidden_dim, mlp_dim, bias = mlp_bias)\n",
    "            mlp_out = nn.Linear(mlp_dim, hidden_dim, bias = mlp_bias)\n",
    "            \n",
    "            if mlp_tied:\n",
    "                assert not mlp_bias\n",
    "                mlp_out.weight = nn.Parameter(mlp_in.weight.transpose(0, 1))\n",
    "            mlp_ins.append(mlp_in)\n",
    "            mlp_outs.append(mlp_out)\n",
    "\n",
    "        self.mlp_ins = nn.ModuleList(mlp_ins)\n",
    "        self.mlp_outs = nn.ModuleList(mlp_outs)\n",
    "        self.n_mlps = n_mlps\n",
    "\n",
    "    def forward(self, x, hooked = False):\n",
    "        if hooked:\n",
    "            activations = {}\n",
    "            activations['res_0'] = self.embedding(x)\n",
    "            for i in range(1,self.n_mlps+1):\n",
    "                activations[f'mlp_in_pre_{i}'] = self.mlp_ins[i-1](activations[f'res_{i-1}'])\n",
    "                activations[f'mlp_in_post_{i}'] = self.nonlinearity(activations[f'mlp_in_pre_{i}'])\n",
    "                activations[f'mlp_out_{i}'] = self.mlp_outs[i-1](activations[f'mlp_in_post_{i}'])\n",
    "                activations[f'res_{i}'] = activations[f'res_{i-1}'] + activations[f'mlp_out_{i}']\n",
    "            activations['unembed_pre'] = self.unembedding(activations[f'res_{self.n_mlps}'])\n",
    "            activations['output'] = self.nonlinearity(activations['unembed_pre'])\n",
    "            return activations['output'], activations\n",
    "\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            for i in range(self.n_mlps):\n",
    "                x = x + self.mlp_outs[i](self.nonlinearity(self.mlp_ins[i](x)))\n",
    "            x = self.unembedding(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(weight_matrix, jitter = 0.05, normalised = False, save = False, epoch = None):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for i in range(weight_matrix.shape[0]):\n",
    "        normalisation = (weight_matrix[i,0]**2 + weight_matrix[i,1]**2) **0.5 if normalised else 1 \n",
    "        plt.arrow(0, 0, weight_matrix[i,0]/normalisation, weight_matrix[i,1]/normalisation, head_width=0.05, head_length=0.1, fc='blue', ec='blue')\n",
    "        plt.text(weight_matrix[i,0]/normalisation + jitter * torch.randn(1), weight_matrix[i,1]/normalisation + jitter * torch.randn(1), f\"{i}\", color='red', fontsize=12)\n",
    "\n",
    "    mins = -1.2 if normalised else weight_matrix.min()-0.5\n",
    "    maxs = 1.2 if normalised else weight_matrix.max()+0.5\n",
    "    plt.xlim(mins,maxs)\n",
    "    plt.ylim(mins,maxs)\n",
    "    plt.grid()\n",
    "    if save:\n",
    "        assert epoch is not None\n",
    "        plt.savefig(f\"weights_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def force_numpy(matrix):\n",
    "    if isinstance(matrix,np.ndarray):\n",
    "        return matrix\n",
    "    elif isinstance(matrix, torch.Tensor):\n",
    "        return matrix.cpu().detach().numpy()\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "def plot_weights_interactive(weights_history, x_dir=None, y_dir=None, z_dir=None, store_rate=1, dotsize = 5, with_labels = True):\n",
    "\n",
    "    for key, weight_list in weights_history.items():\n",
    "        # Initialize figure for each weight list\n",
    "        fig = go.Figure()\n",
    "        weight_list = [force_numpy(weight_matrix) for weight_matrix in weight_list]\n",
    "        max_value = np.max([np.abs(weight_matrix).max() for weight_matrix in weight_list])\n",
    "\n",
    "        # Check if weights are scalars\n",
    "        if weight_list[0].ndim == 0:\n",
    "            plt.plot(weight_list)\n",
    "            continue\n",
    "\n",
    "        weight_shape = min(weight_list[0].shape)\n",
    "        is_bias = True if len(weight_list[0].shape) == 1 else False\n",
    "\n",
    "        if x_dir is None:\n",
    "            x_dir, y_dir, z_dir = (np.zeros(weight_shape), np.zeros(weight_shape), np.zeros(weight_shape))\n",
    "            x_dir[0] = 1\n",
    "            y_dir[1] = 1\n",
    "            if weight_shape == 3:  # Check if vectors are 3d\n",
    "                z_dir[2] = 1\n",
    "\n",
    "        # Create a scatter plot for each weight matrix\n",
    "        for i, weight_matrix in enumerate(weight_list):\n",
    "            if is_bias:\n",
    "                new_matrix = np.zeros((weight_matrix.shape[0],2))\n",
    "                new_matrix[:,0] = weight_matrix\n",
    "                weight_matrix = new_matrix\n",
    "            if weight_matrix.shape[1] > weight_matrix.shape[0]:\n",
    "                weight_matrix = weight_matrix.T \n",
    "\n",
    "            x_values = weight_matrix @ x_dir\n",
    "            y_values = weight_matrix @ y_dir\n",
    "            z_values = weight_matrix @ z_dir if weight_shape == 3 else None  # Calculate z values only for 3d vectors\n",
    "            labels = list(range(len(x_values))) if with_labels else ['' for _ in range(len(x_values))]\n",
    "\n",
    "            # Check if vectors are 2d or 3d\n",
    "            if z_values is None:\n",
    "                scatter = go.Scatter(x=x_values, y=y_values, mode='markers+text', text=labels,\n",
    "                                     textposition='top center', marker=dict(size=dotsize), visible=False, name=f'Epoch {i * store_rate}')\n",
    "            else:\n",
    "                scatter = go.Scatter3d(x=x_values, y=y_values, z=z_values, mode='markers+text', text=labels,\n",
    "                                       marker=dict(size=dotsize), visible=False, name=f'Epoch {i * store_rate}')\n",
    "\n",
    "            fig.add_trace(scatter)\n",
    "\n",
    "        fig.data[0].visible = True\n",
    "        if z_values is not None:\n",
    "                fig.update_layout(scene = dict(\n",
    "                    xaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='X Value'),\n",
    "                    yaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Y Value'),\n",
    "                    zaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Z Value'),\n",
    "                    aspectmode='cube'))\n",
    "        else:\n",
    "            fig.update_xaxes(title_text='X Value', range=[-max_value * 1.1, max_value * 1.1])\n",
    "            fig.update_yaxes(title_text='Y Value', range=[-max_value * 1.1, max_value * 1.1])\n",
    "\n",
    "\n",
    "        # Add a slider for each epoch in the weight list\n",
    "        steps = []\n",
    "        for i in range(len(weight_list)):\n",
    "            step = dict(\n",
    "                method='restyle',\n",
    "                args=['visible', [False] * len(fig.data)],\n",
    "                label=f'Epoch {i * store_rate}'\n",
    "            )\n",
    "            step['args'][1][i] = True  # Toggle i'th trace to \"visible\"\n",
    "            steps.append(step)\n",
    "\n",
    "        slider = dict(\n",
    "            active=0,\n",
    "            currentvalue={\"prefix\": f\"{key} - \"},\n",
    "            pad={\"t\": 50},\n",
    "            steps=steps\n",
    "        )\n",
    "\n",
    "        fig.update_layout(sliders=[slider], width=800, height=800)\n",
    "\n",
    "        # Show figure for current weight list\n",
    "        fig.show()\n",
    "\n",
    "def get_activation_history(model_history, f, included_keys=None):\n",
    "    out, activations = list(model_history.values())[0](torch.eye(f), hooked = True)\n",
    "    if included_keys is None:\n",
    "        activation_history = {k: [] for k in activations}\n",
    "    else:\n",
    "        assert all([k in activations for k in included_keys]), f'Valid keys are {activations.keys()}'\n",
    "        activation_history = {k: [] for k in included_keys}\n",
    "    for model in model_history.values():\n",
    "        out, activations = model(torch.eye(f), hooked = True)\n",
    "        for k in activation_history:\n",
    "            activation_history[k].append(activations[k])\n",
    "    return activation_history\n",
    "\n",
    "def calculate_angles(tensor):\n",
    "    assert tensor.shape[1] == 2, \"Input tensor must be of shape (n, 2)\"\n",
    "    return torch.atan2(tensor[:, 1], tensor[:, 0])\n",
    "\n",
    "def linearity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_interactive(weights_history,\n",
    "                             save_name, \n",
    "                             epochs_saved,\n",
    "                             x_dir=None,\n",
    "                             y_dir=None,\n",
    "                             z_dir=None,\n",
    "                             store_rate=1,\n",
    "                             dotsize=5):\n",
    "    frames = []\n",
    "    fig = make_subplots()\n",
    "\n",
    "    # Create directory to save videos if save_name is provided\n",
    "    save_dir = os.path.join('videos', save_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for key, weight_list in weights_history.items():\n",
    "        # Create directory to save images for each key if save_name is provided\n",
    "        key_dir = os.path.join(save_dir, key)#type: ignore\n",
    "        os.makedirs(key_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize figure for each weight list\n",
    "        fig = go.Figure()\n",
    "        weight_list = [force_numpy(weight_matrix) for weight_matrix in weight_list]\n",
    "        max_value = np.max([np.abs(weight_matrix).max() for weight_matrix in weight_list])\n",
    "\n",
    "        if weight_list[0].ndim == 0:\n",
    "            plt.plot(weight_list)\n",
    "            continue\n",
    "\n",
    "        weight_shape = min(weight_list[0].shape)\n",
    "        is_bias = True if len(weight_list[0].shape) == 1 else False\n",
    "\n",
    "        if x_dir is None:\n",
    "            x_dir, y_dir, z_dir = (np.zeros(weight_shape), np.zeros(weight_shape), np.zeros(weight_shape))\n",
    "            x_dir[0] = 1\n",
    "            y_dir[1] = 1\n",
    "            if weight_shape == 3:  # Check if vectors are 3d\n",
    "                z_dir[2] = 1\n",
    "\n",
    "        # Create a scatter plot for each weight matrix\n",
    "        for i, weight_matrix in enumerate(weight_list):\n",
    "            if not i*store_rate in epochs_saved:\n",
    "                continue\n",
    "            if is_bias:\n",
    "                new_matrix = np.zeros((weight_matrix.shape[0],2))\n",
    "                new_matrix[:,0] = weight_matrix\n",
    "                weight_matrix = new_matrix\n",
    "            if weight_matrix.shape[1] > weight_matrix.shape[0]:\n",
    "                weight_matrix = weight_matrix.T \n",
    "\n",
    "            x_values = weight_matrix @ x_dir\n",
    "            y_values = weight_matrix @ y_dir\n",
    "            z_values = weight_matrix @ z_dir if weight_shape == 3 else None  # Calculate z values only for 3d vectors\n",
    "            labels = list(range(len(x_values)))\n",
    "\n",
    "            # Check if vectors are 2d or 3d\n",
    "            if z_values is None:\n",
    "                scatter = go.Scatter(x=x_values, y=y_values, mode='markers+text', text=labels,\n",
    "                                     textposition='top center', marker=dict(size=dotsize), visible=False, name=f'Epoch {i * store_rate}')\n",
    "            else:\n",
    "                scatter = go.Scatter3d(x=x_values, y=y_values, z=z_values, mode='markers+text', text=labels,\n",
    "                                       marker=dict(size=dotsize), visible=False, name=f'Epoch {i * store_rate}')\n",
    "\n",
    "            fig.add_trace(scatter)\n",
    "\n",
    "        fig.data[0].visible = True\n",
    "        if z_values is not None:\n",
    "            fig.update_layout(scene = dict(\n",
    "                xaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Neuron 1'),\n",
    "                yaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Neuron 2'),\n",
    "                zaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Neuron 3'),\n",
    "                aspectmode='cube'))\n",
    "        else:\n",
    "            fig.update_xaxes(title_text='Neuron 1', range=[-max_value * 1.1, max_value * 1.1])\n",
    "            fig.update_yaxes(title_text='Neuron 2', range=[-max_value * 1.1, max_value * 1.1])\n",
    "        # Add a slider for each epoch in the weight list\n",
    "        steps = []\n",
    "        for i,epoch in enumerate(epochs_saved):\n",
    "            step = dict(\n",
    "                method='restyle',\n",
    "                args=['visible', [False] * len(fig.data)],\n",
    "                label=f'Epoch {epoch}'\n",
    "            )\n",
    "            step['args'][1][i] = True  # Toggle i'th trace to \"visible\"\n",
    "            steps.append(step)\n",
    "            fig.update_traces(visible=False)\n",
    "            fig.data[i].visible = True\n",
    "\n",
    "            # Save image for the epoch if save_name is provided and the epoch is in epochs_saved\n",
    "            fig.write_image(os.path.join(key_dir, f'epoch_{epoch}.png'))\n",
    "\n",
    "        slider = dict(\n",
    "            active=0,\n",
    "            currentvalue={\"prefix\": f\"{key} - \"},\n",
    "            pad={\"t\": 50},\n",
    "            steps=steps\n",
    "        )\n",
    "\n",
    "        fig.update_layout(sliders=[slider], width=800, height=800)\n",
    "\n",
    "        # Show figure for current weight list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, max_lr, decay_factor, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_lr = max_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        super(CustomScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # linear warmup\n",
    "            return [base_lr + self.last_epoch * ((self.max_lr - base_lr) / self.warmup_steps) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # exponential decay\n",
    "            return [self.max_lr * (self.decay_factor ** (self.last_epoch - self.warmup_steps)) for _ in self.base_lrs]\n",
    "\n",
    "\n",
    "def train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler = None, lr_print_rate = 0):\n",
    "    weights_history = {k:[v.detach().numpy().copy()] for k,v in dict(model.named_parameters()).items()}  # Store the weights here\n",
    "    model_history = {} #store model here\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        if logging_loss:\n",
    "            losses.append(avg_loss)\n",
    "            if plot_rate > 0:\n",
    "                if (epoch + 1) % plot_rate == 0:\n",
    "                    plt.figure(figsize=(5,5))\n",
    "                    plt.plot(losses)\n",
    "                    plt.show()\n",
    "        if (epoch + 1) % store_rate == 0:\n",
    "            for k,v in dict(model.named_parameters()).items():\n",
    "                weights_history[k].append(v.detach().numpy().copy())\n",
    "            model_history[epoch] = copy.deepcopy(model)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if lr_print_rate > 0:\n",
    "            if (epoch % lr_print_rate) == 0:\n",
    "                print(optimizer.param_groups[0]['lr'])\n",
    "    return losses, weights_history, model_history  # Return the weights history\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_vectors(vectors, epsilon):\n",
    "    # Store the groups of similar vectors here\n",
    "    groups = []\n",
    "    norms = []\n",
    "    directions = []\n",
    "\n",
    "    for v in vectors:\n",
    "        # Normalize the current vector\n",
    "        v_norm = v / np.linalg.norm(v)\n",
    "        if np.linalg.norm(v) < 0.01:\n",
    "            continue\n",
    "\n",
    "        # This flag will tell us if the current vector has been added to any group\n",
    "        added_to_group = False\n",
    "\n",
    "        # Go through each existing group to check if this vector belongs there\n",
    "        for i,group in enumerate(groups):\n",
    "            # We use the first vector in the group as representative\n",
    "            group_representative = group[0]\n",
    "            group_representative_norm = group_representative / np.linalg.norm(group_representative)\n",
    "\n",
    "            # Calculate the dot product between the normalized vectors\n",
    "            dot_product = np.dot(v_norm, group_representative_norm)\n",
    "\n",
    "            # Check if the dot product is close enough to 1 (indicating they are scalar multiples of each other)\n",
    "            if np.abs(dot_product - 1) < epsilon:\n",
    "                group.append(v)\n",
    "                norms[i].append(v_norm)\n",
    "                added_to_group = True\n",
    "                break\n",
    "\n",
    "        # If the current vector has not been added to any group, we create a new group for it\n",
    "        if not added_to_group:\n",
    "            groups.append([v])\n",
    "            norms.append([v_norm])\n",
    "    \n",
    "    for norm in norms:\n",
    "        arr = np.array(norm)\n",
    "        directions.append(np.mean(arr,axis=0))\n",
    "\n",
    "        \n",
    "\n",
    "    return groups,directions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_matrices_with_slider(matrices, rate, const_colorbar=False):\n",
    "    # Find global min and max if constant colorbar is requested\n",
    "    if const_colorbar:\n",
    "        global_min = np.min([np.min(matrix) for matrix in matrices])\n",
    "        global_max = np.max([np.max(matrix) for matrix in matrices])\n",
    "\n",
    "    # Create empty figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each matrix\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        # Create a heatmap for the matrix\n",
    "        heatmap = go.Heatmap(\n",
    "            z=matrix, \n",
    "            colorscale='magma', \n",
    "            showscale=True,\n",
    "            zmin=global_min if const_colorbar else None,\n",
    "            zmax=global_max if const_colorbar else None\n",
    "        )\n",
    "        fig.update_yaxes(autorange='reversed')\n",
    "        # Add the heatmap to the figure, but only make it visible if it's the first one\n",
    "        fig.add_trace(heatmap)\n",
    "        fig.data[i].visible = (i == 0)\n",
    "        fig.data[i].name = f'Epoch {i * rate}'\n",
    "        \n",
    "    # Create a slider\n",
    "    steps = []\n",
    "    for i in range(len(matrices)):\n",
    "        step = dict(\n",
    "            method=\"restyle\",\n",
    "            args=[\"visible\", [False] * len(matrices)],\n",
    "            label=f'Epoch {i * rate}'\n",
    "        )\n",
    "        step[\"args\"][1][i] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Displaying: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    # Add the slider to the figure\n",
    "    fig.update_layout(\n",
    "        sliders=sliders,\n",
    "        height = 800,\n",
    "        width = 800\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def generate_matrix_list(weights_history):\n",
    "    n = len(weights_history['embedding.weight'])\n",
    "    return [weights_history['unembedding.weight'][i] @ weights_history['embedding.weight'][i] for i in range(n)]\n",
    "\n",
    "def np_gelu(matrix):\n",
    "    return F.gelu(torch.tensor(matrix)).detach().numpy()\n",
    "\n",
    "def nonlinearity_numpy(matrix, nonlinearity):\n",
    "    return nonlinearity(torch.tensor(matrix)).detach().numpy()\n",
    "\n",
    "\n",
    "def filter_to_convex_hull(points):\n",
    "    # Convert points to numpy array\n",
    "    points = np.array(points)\n",
    "    \n",
    "    # Calculate the convex hull of the points\n",
    "    hull = ConvexHull(points)\n",
    "    \n",
    "    # Return only the points that are vertices of the hull\n",
    "    hull_points = points[hull.vertices]\n",
    "    \n",
    "    # Convert back to python list before returning\n",
    "    return hull_points.tolist()\n",
    "\n",
    "def visualise_polyhedron(vertices, filled_faces = True, opacity = 1, with_labels = False):\n",
    "    # convert vertices list to numpy array for convenience\n",
    "    if vertices.shape[1] > vertices.shape[0]:\n",
    "                vertices = vertices.T \n",
    "    \n",
    "    # scipy's ConvexHull will give us the simplices (triangles) that form the polyhedron\n",
    "    from scipy.spatial import ConvexHull\n",
    "    hull = ConvexHull(vertices)\n",
    "    \n",
    "    # initialize 3D plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # add each simplex as a triangular face\n",
    "    # add each simplex as a triangular face\n",
    "    for s in hull.simplices:\n",
    "        # Ensure vertices are in counterclockwise order\n",
    "        cross_product = np.cross(vertices[s[1]] - vertices[s[0]], vertices[s[2]] - vertices[s[0]])\n",
    "        dot_product = np.dot(cross_product, hull.equations[s[0], :-1])\n",
    "        \n",
    "        if dot_product < 0:\n",
    "            s = s[[0, 2, 1]]  # Swap the last two elements to change the order to counterclockwise\n",
    "        \n",
    "        s = np.append(s, s[0])  # Here we cycle back to the first coordinate for plotly\n",
    "        if filled_faces:\n",
    "            x = vertices[s, 0]\n",
    "            y = vertices[s, 1]\n",
    "            z = vertices[s, 2]\n",
    "            fig.add_trace(go.Mesh3d(x=x, y=y, z=z, color='lightpink', opacity=opacity))\n",
    "        fig.add_trace(go.Scatter3d(x=vertices[s, 0], y=vertices[s, 1], z=vertices[s, 2],\n",
    "                                mode='lines',\n",
    "                                line=dict(color='blue', width=2)))\n",
    "\n",
    "    # add each vertex in the hull as a label\n",
    "    if with_labels:\n",
    "        for i in hull.vertices:\n",
    "            fig.add_trace(go.Scatter3d(x=[vertices[i, 0]], y=[vertices[i, 1]], z=[vertices[i, 2]],\n",
    "                                    mode='text',\n",
    "                                    text=[str(i)],  # or other string labels\n",
    "                                    textposition='top center'))\n",
    "\n",
    "        \n",
    "    # set the 3d scene parameters\n",
    "    fig.update_layout(showlegend = False,\n",
    "                      scene = dict(xaxis_title='X',\n",
    "                                   yaxis_title='Y',\n",
    "                                   zaxis_title='Z',\n",
    "                                   aspectmode='auto'),\n",
    "                      width=700,\n",
    "                      margin=dict(r=20, l=10, b=10, t=10))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def smallest_angle_vectors(list1, list2):\n",
    "    result = {}\n",
    "    \n",
    "    for i, v in enumerate(list1):\n",
    "        smallest_angle = None\n",
    "        smallest_index = None\n",
    "        \n",
    "        for j, u in enumerate(list2):\n",
    "            cosine_angle = np.dot(v, u) / (np.linalg.norm(v) * np.linalg.norm(u))\n",
    "            angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))  # ensure value is in the valid domain for arccos\n",
    "            \n",
    "            if smallest_angle is None or angle < smallest_angle:\n",
    "                smallest_angle = angle\n",
    "                smallest_index = j\n",
    "                \n",
    "        result[i] = (smallest_index, np.degrees(smallest_angle))  # convert angle to degrees\n",
    "\n",
    "    return result\n",
    "\n",
    "def relu_plusone(x):\n",
    "    return F.relu(x+1)\n",
    "\n",
    "def relu_minusone(x):\n",
    "    return F.relu(x-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 0 Layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 500\n",
    "set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "\n",
    "#Define the Loss function\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 30\n",
    "k = 1\n",
    "n = 2\n",
    "nonlinearity = relu_plusone\n",
    "tied = True\n",
    "final_bias = True\n",
    "unit_weights = False\n",
    "with_scale_factor = unit_weights\n",
    "standard_magnitude = False\n",
    "\n",
    "\n",
    "epochs = 1000000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 10\n",
    "initial_lr = 10\n",
    "warmup_frac = 0.05\n",
    "final_lr = 0.1\n",
    "decay_factor=(final_lr/max_lr)**(1/(epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            with_scale_factor=with_scale_factor,\n",
    "            standard_magnitude=standard_magnitude)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.array(losses)-min(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(calculate_angles(model.embedding.weight.T).detach().numpy(), bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_history = get_activation_history(model_history,f,['res_pre'])\n",
    "plot_weights_interactive(activation_history,store_rate=store_rate, with_labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_interactive(weights_history, store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.norm(model.embedding.weight, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.203*np.cos(72*np.pi/180) * 1.203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_history = {'embedding.weight': weights_history['embedding.weight']}\n",
    "save_weights_interactive(embeddings_history,\n",
    "                         save_name = 'n2_f20_tied_nobias_unnormalised_epochs10k_seed242',\n",
    "                         epochs_saved = list(range(0,2400,400)),\n",
    "                         store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([m.scale_factor.data.detach().numpy() for m in model_history.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(i,bias.item()) for i,bias in enumerate(model.unembedding.bias)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_vec = torch.zeros(f)\n",
    "random_vec[1] = 1\n",
    "for i,(v1,v2) in enumerate(zip(einsum('i j, j k, k -> i', model.unembedding.weight, model.embedding.weight, random_vec) + model.unembedding.bias, model(random_vec))):\n",
    "    a = model.unembedding.weight[i] @ model.embedding.weight @ random_vec + model.unembedding.bias[i]\n",
    "    print(a.item(),v1.item(),v2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, directions = group_vectors(model.unembedding.weight.data.detach().numpy(),0.00001)\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = model.unembedding.weight.data.norm(dim = 1).detach().numpy()\n",
    "[F.gelu(torch.tensor(m**2)) for m in magnitudes if m > 0.001]\n",
    "magnitudes = []\n",
    "for group in groups:\n",
    "    norms = np.zeros(len(group))\n",
    "    for i,elem in enumerate(group):\n",
    "        norms[i] = np.linalg.norm(elem)\n",
    "    magnitudes.append(norms.mean())\n",
    "[m for m in magnitudes if m> 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_interactive(weights_history,store_rate=store_rate, video_save_name = 'n2_f20_tied_nobias_unnormalised_epochs10k_seed242')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_angle_vectors(model.embedding.weight.detach().numpy().T, model.unembedding.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_relu = [model(torch.eye(f)).cpu().detach().numpy() for model in model_history.values()]\n",
    "post_softmax = [model(torch.eye(f)).softmax(dim=1).cpu().detach().numpy() for model in model_history.values()]\n",
    "pre_relu = []\n",
    "for model in model_history.values():\n",
    "    out, activations = model(torch.eye(f), hooked=True)\n",
    "    pre_relu.append(activations['unembed_pre'].cpu().detach().numpy())\n",
    "if criterion == nn.CrossEntropyLoss():\n",
    "    visualize_matrices_with_slider(post_softmax, store_rate, const_colorbar=True)\n",
    "visualize_matrices_with_slider(post_relu, store_rate, const_colorbar=True)\n",
    "visualize_matrices_with_slider(pre_relu, store_rate, const_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, directions = group_vectors(model.unembedding.weight.data.detach().numpy(),0.00001)\n",
    "directions = torch.tensor(np.array(directions))\n",
    "\n",
    "visualise_polyhedron(model.unembedding.weight.data, with_labels= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 1\n",
    "\n",
    "set_seed(chosen_seed)\n",
    "\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 20\n",
    "n = 2\n",
    "nonlinearity =  nn.GELU()\n",
    "tied = False\n",
    "mlp_tied = tied\n",
    "final_bias = False\n",
    "mlp_bias = False\n",
    "n_mlps = 1\n",
    "\n",
    "\n",
    "epochs = 80000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 1\n",
    "initial_lr = 0.005\n",
    "warmup_frac = 0.05\n",
    "final_lr = 0.1\n",
    "decay_factor=(final_lr/max_lr)**(1/(epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs/500\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = SyntheticNormalised(f)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet(f, n, 4*n,\n",
    "               tied = tied,\n",
    "               mlp_tied = mlp_tied,\n",
    "               final_bias = final_bias,\n",
    "               mlp_bias = mlp_bias,\n",
    "               nonlinearity=nonlinearity,\n",
    "               n_mlps = n_mlps)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_interactive(weights_history,store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_visualise = [f'res_{i}' for i in range(n_mlps+1)]\n",
    "activation_history = get_activation_history(model_history,f,to_visualise)\n",
    "plot_weights_interactive(activation_history,store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_list = [model(torch.eye(f)).cpu().detach().numpy() for model in model_history.values()]\n",
    "pre_relu = []\n",
    "for model in model_history.values():\n",
    "    out, activations = model(torch.eye(f), hooked=True)\n",
    "    pre_relu.append(activations['unembed_pre'].cpu().detach().numpy())\n",
    "visualize_matrices_with_slider(matrix_list, store_rate, const_colorbar=True)\n",
    "visualize_matrices_with_slider(pre_relu, store_rate, const_colorbar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
