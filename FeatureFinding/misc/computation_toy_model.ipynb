{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib as mpl\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from fancy_einsum import einsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "fontsize = 20\n",
    "mpl.rcParams['font.size'] = fontsize\n",
    "mpl.rcParams['xtick.labelsize'] = fontsize\n",
    "mpl.rcParams['ytick.labelsize'] = fontsize\n",
    "mpl.rcParams['legend.fontsize'] = fontsize\n",
    "mpl.rcParams['axes.titlesize'] = fontsize\n",
    "mpl.rcParams['axes.labelsize'] = fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, max_lr, decay_factor, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_lr = max_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        super(CustomScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # linear warmup\n",
    "            return [base_lr + self.last_epoch * ((self.max_lr - base_lr) / self.warmup_steps) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # exponential decay\n",
    "            return [self.max_lr * (self.decay_factor ** (self.last_epoch - self.warmup_steps)) for _ in self.base_lrs]\n",
    "\n",
    "\n",
    "def train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler = None, lr_print_rate = 0):\n",
    "    weights_history = {k:[v.detach().numpy().copy()] for k,v in dict(model.named_parameters()).items()}  # Store the weights here\n",
    "    model_history = {} #store model here\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        if logging_loss:\n",
    "            losses.append(avg_loss)\n",
    "            if plot_rate > 0:\n",
    "                if (epoch + 1) % plot_rate == 0:\n",
    "                    plt.figure(figsize=(5,5))\n",
    "                    plt.plot(losses)\n",
    "                    plt.show()\n",
    "        if (epoch + 1) % store_rate == 0:\n",
    "            for k,v in dict(model.named_parameters()).items():\n",
    "                weights_history[k].append(v.detach().numpy().copy())\n",
    "            model_history[epoch] = copy.deepcopy(model)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if lr_print_rate > 0:\n",
    "            if (epoch % lr_print_rate) == 0:\n",
    "                print(optimizer.param_groups[0]['lr'])\n",
    "    return losses, weights_history, model_history  # Return the weights history\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_vectors(vectors, epsilon):\n",
    "    # Store the groups of similar vectors here\n",
    "    groups = []\n",
    "    norms = []\n",
    "    directions = []\n",
    "\n",
    "    for v in vectors:\n",
    "        # Normalize the current vector\n",
    "        v_norm = v / np.linalg.norm(v)\n",
    "        if np.linalg.norm(v) < 0.01:\n",
    "            continue\n",
    "\n",
    "        # This flag will tell us if the current vector has been added to any group\n",
    "        added_to_group = False\n",
    "\n",
    "        # Go through each existing group to check if this vector belongs there\n",
    "        for i,group in enumerate(groups):\n",
    "            # We use the first vector in the group as representative\n",
    "            group_representative = group[0]\n",
    "            group_representative_norm = group_representative / np.linalg.norm(group_representative)\n",
    "\n",
    "            # Calculate the dot product between the normalized vectors\n",
    "            dot_product = np.dot(v_norm, group_representative_norm)\n",
    "\n",
    "            # Check if the dot product is close enough to 1 (indicating they are scalar multiples of each other)\n",
    "            if np.abs(dot_product - 1) < epsilon:\n",
    "                group.append(v)\n",
    "                norms[i].append(v_norm)\n",
    "                added_to_group = True\n",
    "                break\n",
    "\n",
    "        # If the current vector has not been added to any group, we create a new group for it\n",
    "        if not added_to_group:\n",
    "            groups.append([v])\n",
    "            norms.append([v_norm])\n",
    "    \n",
    "    for norm in norms:\n",
    "        arr = np.array(norm)\n",
    "        directions.append(np.mean(arr,axis=0))\n",
    "\n",
    "        \n",
    "\n",
    "    return groups,directions\n",
    "\n",
    "\n",
    "def visualize_matrices_with_slider(matrices, rate, const_colorbar=False):\n",
    "    # Find global min and max if constant colorbar is requested\n",
    "    if const_colorbar:\n",
    "        global_min = np.min([np.min(matrix) for matrix in matrices])\n",
    "        global_max = np.max([np.max(matrix) for matrix in matrices])\n",
    "\n",
    "    # Create empty figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each matrix\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        # Create a heatmap for the matrix\n",
    "        heatmap = go.Heatmap(\n",
    "            z=matrix, \n",
    "            colorscale='magma', \n",
    "            showscale=True,\n",
    "            zmin=global_min if const_colorbar else None,\n",
    "            zmax=global_max if const_colorbar else None\n",
    "        )\n",
    "        fig.update_yaxes(autorange='reversed')\n",
    "        # Add the heatmap to the figure, but only make it visible if it's the first one\n",
    "        fig.add_trace(heatmap)\n",
    "        fig.data[i].visible = (i == 0)\n",
    "        fig.data[i].name = f'Epoch {i * rate}'\n",
    "        \n",
    "    # Create a slider\n",
    "    steps = []\n",
    "    for i in range(len(matrices)):\n",
    "        step = dict(\n",
    "            method=\"restyle\",\n",
    "            args=[\"visible\", [False] * len(matrices)],\n",
    "            label=f'Epoch {i * rate}'\n",
    "        )\n",
    "        step[\"args\"][1][i] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Displaying: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    # Add the slider to the figure\n",
    "    fig.update_layout(\n",
    "        sliders=sliders,\n",
    "        height = 800,\n",
    "        width = 800\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def generate_matrix_list(weights_history):\n",
    "    n = len(weights_history['embedding.weight'])\n",
    "    return [weights_history['unembedding.weight'][i] @ weights_history['embedding.weight'][i] for i in range(n)]\n",
    "\n",
    "def np_gelu(matrix):\n",
    "    return F.gelu(torch.tensor(matrix)).detach().numpy()\n",
    "\n",
    "def nonlinearity_numpy(matrix, nonlinearity):\n",
    "    return nonlinearity(torch.tensor(matrix)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_samples, f):\n",
    "        self.num_samples = num_samples\n",
    "        self.f = f\n",
    "        self.data = self.generate_data()\n",
    "        \n",
    "    def generate_data(self):\n",
    "        data = torch.zeros((self.num_samples, self.f))\n",
    "        for i in range(self.num_samples):\n",
    "            index = torch.randint(0, self.f, (1,))\n",
    "            data[i, index] = torch.rand(1)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SyntheticNormalised(Dataset):\n",
    "    #Creates a dataset with f 1-hot vectors as the dataset.\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "        self.data = self.generate_data()\n",
    "        \n",
    "    def generate_data(self):\n",
    "        return torch.eye(self.f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.f\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class SyntheticKHot(Dataset):\n",
    "    def __init__(self, f, k):\n",
    "        self.f = f\n",
    "        self.k = k\n",
    "        self.data = []\n",
    "\n",
    "        # Create all possible combinations of f choose k\n",
    "        for indices in combinations(range(f), k):\n",
    "            vec = torch.zeros(f)\n",
    "            vec[list(indices)] = 1\n",
    "            self.data.append(vec)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        return self.data[idx]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, tied = True, final_bias = False, nonlinearity = F.relu, unit_weights=False, with_scale_factor = False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.tied = tied\n",
    "        self.final_bias = final_bias\n",
    "        self.unit_weights = unit_weights\n",
    "        self.with_scale_factor = with_scale_factor\n",
    "\n",
    "\n",
    "        # Define the input layer (embedding)\n",
    "        self.embedding = nn.Linear(self.input_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        # Define the output layer (unembedding)\n",
    "        self.unembedding = nn.Linear(self.hidden_dim, self.input_dim, bias=final_bias)\n",
    "\n",
    "        if self.unit_weights:\n",
    "            # Normalize the weight to have unit norm for each row\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "        # Tie the weights\n",
    "        if tied:\n",
    "            self.unembedding.weight = torch.nn.Parameter(self.embedding.weight.transpose(0, 1))\n",
    "\n",
    "        if self.with_scale_factor:\n",
    "            self.scale_factor = nn.Parameter(torch.tensor(1.0))\n",
    "        else:\n",
    "            self.scale_factor = 1.0\n",
    "\n",
    "    def forward(self, x, hooked = False):\n",
    "        if self.unit_weights:\n",
    "            # Normalize the weight to have unit norm for each row\n",
    "            self.embedding.weight.data = F.normalize(self.embedding.weight.data, p=2, dim=0)\n",
    "            if self.tied:\n",
    "                self.unembedding.weight.data = self.embedding.weight.data.transpose(0, 1)\n",
    "        if hooked:\n",
    "            activations = {}\n",
    "            activations['res_pre'] = self.embedding(x)\n",
    "            activations['unembed_pre'] = self.unembedding(activations['res_pre'])\n",
    "            activations['output'] = self.scale_factor * self.nonlinearity(activations['unembed_pre'])\n",
    "            return activations['output'], activations\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = self.unembedding(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            return self.scale_factor * x\n",
    "        \n",
    "class ResNet(Net):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 mlp_dim,\n",
    "                 tied = True,\n",
    "                 mlp_tied = True,\n",
    "                 mlp_bias = False,\n",
    "                 final_bias = False,\n",
    "                 nonlinearity = F.relu,\n",
    "                 n_mlps = 1):\n",
    "        super().__init__(input_dim, hidden_dim, tied, final_bias, nonlinearity)\n",
    "\n",
    "        mlp_ins = []\n",
    "        mlp_outs = []\n",
    "        for i in range(n_mlps):\n",
    "            mlp_in = nn.Linear(hidden_dim, mlp_dim, bias = mlp_bias)\n",
    "            mlp_out = nn.Linear(mlp_dim, hidden_dim, bias = mlp_bias)\n",
    "            \n",
    "            if mlp_tied:\n",
    "                assert not mlp_bias\n",
    "                mlp_out.weight = nn.Parameter(mlp_in.weight.transpose(0, 1))\n",
    "            mlp_ins.append(mlp_in)\n",
    "            mlp_outs.append(mlp_out)\n",
    "\n",
    "        self.mlp_ins = nn.ModuleList(mlp_ins)\n",
    "        self.mlp_outs = nn.ModuleList(mlp_outs)\n",
    "        self.n_mlps = n_mlps\n",
    "\n",
    "    def forward(self, x, hooked = False):\n",
    "        if hooked:\n",
    "            activations = {}\n",
    "            activations['res_0'] = self.embedding(x)\n",
    "            for i in range(1,self.n_mlps+1):\n",
    "                activations[f'mlp_in_pre_{i}'] = self.mlp_ins[i-1](activations[f'res_{i-1}'])\n",
    "                activations[f'mlp_in_post_{i}'] = self.nonlinearity(activations[f'mlp_in_pre_{i}'])\n",
    "                activations[f'mlp_out_{i}'] = self.mlp_outs[i-1](activations[f'mlp_in_post_{i}'])\n",
    "                activations[f'res_{i}'] = activations[f'res_{i-1}'] + activations[f'mlp_out_{i}']\n",
    "            activations['unembed_pre'] = self.unembedding(activations[f'res_{self.n_mlps}'])\n",
    "            activations['output'] = self.nonlinearity(activations['unembed_pre'])\n",
    "            return activations['output'], activations\n",
    "\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            for i in range(self.n_mlps):\n",
    "                x = x + self.mlp_outs[i](self.nonlinearity(self.mlp_ins[i](x)))\n",
    "            x = self.unembedding(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            return x\n",
    "\n",
    "def plot_weights(weight_matrix, jitter = 0.05, normalised = False, save = False, epoch = None):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for i in range(weight_matrix.shape[0]):\n",
    "        normalisation = (weight_matrix[i,0]**2 + weight_matrix[i,1]**2) **0.5 if normalised else 1 \n",
    "        plt.arrow(0, 0, weight_matrix[i,0]/normalisation, weight_matrix[i,1]/normalisation, head_width=0.05, head_length=0.1, fc='blue', ec='blue')\n",
    "        plt.text(weight_matrix[i,0]/normalisation + jitter * torch.randn(1), weight_matrix[i,1]/normalisation + jitter * torch.randn(1), f\"{i}\", color='red', fontsize=12)\n",
    "\n",
    "    mins = -1.2 if normalised else weight_matrix.min()-0.5\n",
    "    maxs = 1.2 if normalised else weight_matrix.max()+0.5\n",
    "    plt.xlim(mins,maxs)\n",
    "    plt.ylim(mins,maxs)\n",
    "    plt.grid()\n",
    "    if save:\n",
    "        assert epoch is not None\n",
    "        plt.savefig(f\"weights_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def normalise(matrix, tolerance = 1e-10):\n",
    "    out = np.zeros_like(matrix)\n",
    "    print(einsum('i j, i j -> i', matrix, matrix))\n",
    "    norms = np.maximum(einsum('i j, i j -> i', matrix, matrix)**0.5,tolerance)\n",
    "    print(norms)\n",
    "    return np.divide(matrix,norms, axis = 1)\n",
    "    # for i,row in enumerate(matrix):\n",
    "    #     norm = (row.T @ row) ** 0.5\n",
    "    #     if norm > tolerance:\n",
    "    #         out[i] = row / norm\n",
    "    #     else:\n",
    "    #         continue\n",
    "    # return out\n",
    "\n",
    "def force_numpy(matrix):\n",
    "    if isinstance(matrix,np.ndarray):\n",
    "        return matrix\n",
    "    elif isinstance(matrix, torch.Tensor):\n",
    "        return matrix.cpu().detach().numpy()\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "def plot_weights_interactive(weights_history, x_dir=None, y_dir=None, z_dir=None, store_rate=1, dotsize = 5):\n",
    "\n",
    "    for key, weight_list in weights_history.items():\n",
    "        # Initialize figure for each weight list\n",
    "        fig = go.Figure()\n",
    "        weight_list = [force_numpy(weight_matrix) for weight_matrix in weight_list]\n",
    "        max_value = np.max([np.abs(weight_matrix).max() for weight_matrix in weight_list])\n",
    "\n",
    "        # Check if weights are scalars\n",
    "        if weight_list[0].ndim == 0:\n",
    "            # Convert scalars to 1-D arrays\n",
    "            weight_list = [np.array([weight_matrix]) for weight_matrix in weight_list]\n",
    "\n",
    "\n",
    "        weight_shape = min(weight_list[0].shape)\n",
    "        is_bias = True if len(weight_list[0].shape) == 1 else False\n",
    "\n",
    "        if x_dir is None:\n",
    "            x_dir, y_dir, z_dir = (np.zeros(weight_shape), np.zeros(weight_shape), np.zeros(weight_shape))\n",
    "            x_dir[0] = 1\n",
    "            y_dir[1] = 1\n",
    "            if weight_shape == 3:  # Check if vectors are 3d\n",
    "                z_dir[2] = 1\n",
    "\n",
    "        # Create a scatter plot for each weight matrix\n",
    "        for i, weight_matrix in enumerate(weight_list):\n",
    "            if is_bias:\n",
    "                new_matrix = np.zeros((weight_matrix.shape[0],2))\n",
    "                new_matrix[:,0] = weight_matrix\n",
    "                weight_matrix = new_matrix\n",
    "            if weight_matrix.shape[1] > weight_matrix.shape[0]:\n",
    "                weight_matrix = weight_matrix.T \n",
    "\n",
    "            x_values = weight_matrix @ x_dir\n",
    "            y_values = weight_matrix @ y_dir\n",
    "            z_values = weight_matrix @ z_dir if weight_shape == 3 else None  # Calculate z values only for 3d vectors\n",
    "            labels = list(range(len(x_values)))\n",
    "\n",
    "            # Check if vectors are 2d or 3d\n",
    "            if z_values is None:\n",
    "                scatter = go.Scatter(x=x_values, y=y_values, mode='markers+text', text=labels,\n",
    "                                     textposition='top center', marker=dict(size=dotsize), visible=False, name=f'Epoch {i * store_rate}')\n",
    "            else:\n",
    "                scatter = go.Scatter3d(x=x_values, y=y_values, z=z_values, mode='markers+text', text=labels,\n",
    "                                       marker=dict(size=dotsize), visible=False, name=f'Epoch {i * store_rate}')\n",
    "\n",
    "            fig.add_trace(scatter)\n",
    "\n",
    "        fig.data[0].visible = True\n",
    "        if z_values is not None:\n",
    "                fig.update_layout(scene = dict(\n",
    "                    xaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='X Value'),\n",
    "                    yaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Y Value'),\n",
    "                    zaxis=dict(range=[-max_value * 1.1,max_value * 1.1], title='Z Value'),\n",
    "                    aspectmode='cube'))\n",
    "        else:\n",
    "            fig.update_xaxes(title_text='X Value', range=[-max_value * 1.1, max_value * 1.1])\n",
    "            fig.update_yaxes(title_text='Y Value', range=[-max_value * 1.1, max_value * 1.1])\n",
    "\n",
    "\n",
    "        # Add a slider for each epoch in the weight list\n",
    "        steps = []\n",
    "        for i in range(len(weight_list)):\n",
    "            step = dict(\n",
    "                method='restyle',\n",
    "                args=['visible', [False] * len(fig.data)],\n",
    "                label=f'Epoch {i * store_rate}'\n",
    "            )\n",
    "            step['args'][1][i] = True  # Toggle i'th trace to \"visible\"\n",
    "            steps.append(step)\n",
    "\n",
    "        slider = dict(\n",
    "            active=0,\n",
    "            currentvalue={\"prefix\": f\"{key} - \"},\n",
    "            pad={\"t\": 50},\n",
    "            steps=steps\n",
    "        )\n",
    "\n",
    "        fig.update_layout(sliders=[slider], width=800, height=800)\n",
    "\n",
    "        # Show figure for current weight list\n",
    "        fig.show()\n",
    "\n",
    "def get_activation_history(model_history, f, included_keys=None):\n",
    "    out, activations = list(model_history.values())[0](torch.eye(f), hooked = True)\n",
    "    if included_keys is None:\n",
    "        activation_history = {k: [] for k in activations}\n",
    "    else:\n",
    "        assert all([k in activations for k in included_keys]), f'Valid keys are {activations.keys()}'\n",
    "        activation_history = {k: [] for k in included_keys}\n",
    "    for model in model_history.values():\n",
    "        out, activations = model(torch.eye(f), hooked = True)\n",
    "        for k in activation_history:\n",
    "            activation_history[k].append(activations[k])\n",
    "    return activation_history\n",
    "\n",
    "def calculate_angles(tensor):\n",
    "    assert tensor.shape[1] == 2, \"Input tensor must be of shape (n, 2)\"\n",
    "    return torch.atan2(tensor[:, 1], tensor[:, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
