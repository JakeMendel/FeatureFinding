{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objs as goa\n",
    "import matplotlib as mpl\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/Users/jakemendel/Desktop/Code/FeatureFinding\") \n",
    "from FeatureFinding import utils, datasets, models\n",
    "from torch.autograd import grad\n",
    "from typing import Dict, List, Optional, Union, Any, Callable\n",
    "import importlib\n",
    "mpl.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "fontsize = 20\n",
    "mpl.rcParams['font.size'] = fontsize\n",
    "mpl.rcParams['xtick.labelsize'] = fontsize\n",
    "mpl.rcParams['ytick.labelsize'] = fontsize\n",
    "mpl.rcParams['legend.fontsize'] = fontsize\n",
    "mpl.rcParams['axes.titlesize'] = fontsize\n",
    "mpl.rcParams['axes.labelsize'] = fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "# your chosen seed\n",
    "chosen_seed = 2\n",
    "utils.set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 40\n",
    "k = 1\n",
    "n = 2\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = utils.relu_plusone\n",
    "tied = True\n",
    "final_bias = True\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "\n",
    "\n",
    "epochs = 40000\n",
    "total_epochs = 300000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 5\n",
    "initial_lr = 0.001\n",
    "warmup_frac = 0.05\n",
    "final_lr = 2\n",
    "decay_factor=(final_lr/max_lr)**(1/(total_epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(total_epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//400\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = datasets.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset)//2 #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "model = models.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = utils.train(model,\n",
    "                                                     loader,criterion,\n",
    "                                                     optimizer,\n",
    "                                                     epochs,\n",
    "                                                     logging_loss,\n",
    "                                                     plot_rate, \n",
    "                                                     store_rate,\n",
    "                                                     scheduler,\n",
    "                                                     lr_print_rate,\n",
    "                                                     dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[1000:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_dict = {i: loss for i, loss in enumerate(losses) if i >=1000}\n",
    "utils.plot_weights_static(weights_history, loss_dict,store_rate=store_rate, epochs_to_show = [1000,17000,6000,8000,13500,16400,25000,16700,39900],to_label = [12,24,31],scale = 1.6,num_across=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_model = model_history[27000]\n",
    "np.dot(past_model.embedding.weight.T[10].detach().numpy(), past_model.embedding.weight.T[2].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history32 = {k: model.float() for k, model in model_history.items()}\n",
    "post_relu = [model(torch.eye(f)).cpu().detach().numpy() for model in model_history32.values()]\n",
    "post_softmax = [model(torch.eye(f)).softmax(dim=1).cpu().detach().numpy() for model in model_history32.values()]\n",
    "pre_relu = []\n",
    "for model in model_history.values():\n",
    "    out, activations = model(torch.eye(f), hooked=True)\n",
    "    pre_relu.append(activations['unembed_pre'].cpu().detach().numpy())\n",
    "if not MSE:\n",
    "    utils.visualize_matrices_with_slider(post_softmax, store_rate, const_colorbar=True)\n",
    "utils.visualize_matrices_with_slider([p for p in post_relu], store_rate, const_colorbar=True, plot_size = 800)\n",
    "utils.visualize_matrices_with_slider(pre_relu, store_rate, const_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "r2_data = {}\n",
    "b_data = {}\n",
    "r2_b_data = {}\n",
    "for epoch, model in model_history.items():\n",
    "    weights = model.embedding.weight.T[torch.norm(model.embedding.weight,dim=0)>0.1]\n",
    "    b = model.unembedding_bias[torch.norm(model.embedding.weight,dim=0)>0.1]+1\n",
    "    r2 = torch.norm(weights,dim=1)**2\n",
    "    r2_data[epoch] = r2.detach().numpy()\n",
    "    b_data[epoch] = b.detach().numpy()\n",
    "    r2_b_data[epoch] = (r2+b).detach().numpy()\n",
    "\n",
    "utils.plot_histograms(r2_data,bins=40)\n",
    "utils.plot_histograms(b_data,bins=40)\n",
    "utils.plot_histograms(r2_b_data,bins=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r2_b_data.keys(),[np.mean(nums) for nums in r2_b_data.values()], label = '$r^2+b$')\n",
    "# plt.plot(r2_b_data.keys(),[np.mean(r2s-bs) for r2s,bs in zip(r2_data.values(),b_data.values())], label = '$r^2-b$')\n",
    "\n",
    "# plt.plot(r2_data.keys(),[np.mean(nums**2) for nums in r2_data.values()], label = '$r^2$')\n",
    "# plt.plot(b_data.keys(),[np.mean(nums) for nums in b_data.values()], label = '$b$')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "reduced_model_history = {k:v.to(dtype = torch.float) for k,v in model_history.items() if k%500 == 0}\n",
    "final_layer_dict = utils.final_layer_dict(reduced_model_history)\n",
    "utils.visualise_activation_space_history(final_layer_dict, n_points = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "interference_data = {}\n",
    "dot_with = 12\n",
    "ignore = [12,24,31]\n",
    "for epoch, model in tqdm(model_history.items()):\n",
    "    weights = model.embedding.weight.T.detach().numpy()\n",
    "    b = model.unembedding_bias.detach().numpy()+1\n",
    "    special_b = b[dot_with]\n",
    "    special_w = weights[dot_with]\n",
    "    interference = 0\n",
    "    interfering_row = None\n",
    "    for i, row in enumerate(weights):\n",
    "        if np.linalg.norm(row) > 0.1:\n",
    "            if i not in ignore:\n",
    "                new_interference = np.dot(special_w, row) + max(special_b,b[i])\n",
    "                if new_interference > interference:\n",
    "                    interference = new_interference\n",
    "                    interfering_row = i\n",
    "    interference_data[epoch] = interference, interfering_row\n",
    "\n",
    "responsible_indices = set([i[1] for i in interference_data.values()])\n",
    "for index in responsible_indices:\n",
    "    x = []\n",
    "    y = []\n",
    "    for i,(epoch,datum) in enumerate(interference_data.items()):\n",
    "        if datum[1] == index:\n",
    "            x.append(epoch)\n",
    "            y.append(datum[0])\n",
    "    plt.semilogy(x,y, label = f'Interference with {index}')\n",
    "plt.legend()\n",
    "plt.ylabel('Max Interference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsible_indices = set([i[1] for i in interference_data.values()])\n",
    "plt.figure()\n",
    "for index in responsible_indices:\n",
    "    x = []\n",
    "    y = []\n",
    "    for i,(epoch,datum) in enumerate(interference_data.items()):\n",
    "        if datum[1] == index:\n",
    "            x.append(epoch)\n",
    "            y.append(datum[0])\n",
    "    plt.semilogy(x,y, label = f'Interference with {index}')\n",
    "plt.legend()\n",
    "plt.ylabel('Max Interference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "for epoch, interferences in interference_data.items():\n",
    "    for i,interference in enumerate(interferences):\n",
    "        if i in sequences.keys():\n",
    "            sequences[i]['epochs'].append(epoch)\n",
    "            sequences[i]['vals'].append(interference)\n",
    "        else:\n",
    "            sequences[i] = {'epochs': [epoch], 'vals': [interference]}\n",
    "for i, data in sequences.items():\n",
    "    plt.plot(data['epochs'],data['vals'], label = f'$W_{10}\\cdot W_{i} + b_{i}$')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3,4]).tolist() in [i.tolist() for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups, directions = utils.group_vectors(model_history[10000].embedding.weight.T[torch.norm(model_history[10000].embedding.weight,dim=0)>0.1].detach().numpy(),0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = np.array([len(g) for g in groups])\n",
    "np.sum(va)/np.sum(va**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat((torch.tensor(np.array(directions)), torch.tensor(np.array(directions)))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.calculate_angles(torch.concat((torch.tensor(np.array(directions)), torch.tensor(np.array(directions)))).T) * 180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history[10000].embedding.weight[torch.norm(model_history[10000].embedding.weight,dim=0)>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_history.values())[-1].unembedding_bias + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(model_history[10000].embedding.weight,dim=0)**2 + model_history[10000].unembedding_bias + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(model_history.keys())[10:], [np.trace(matrix) for matrix in post_relu][10:], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "differences = np.array([np.linalg.norm(matrix[:,31]-matrix[:,12]) for matrix in weights_history['embedding.weight']])\n",
    "plt.semilogy(model_history.keys(), differences)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Distance between feature vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "for b in loader:\n",
    "    batch = b\n",
    "assert isinstance(batch, torch.Tensor)\n",
    "batch32 = batch.float()\n",
    "model_history32 = {k: model.float() for k, model in model_history.items()}\n",
    "model_history_hessians = {k:v for k,v in model_history32.items()}\n",
    "\n",
    "hessians_dict, eigenvalues_dict = utils.calculate_hessians(model_history_hessians,batch32,batch32,nn.MSELoss()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not including embedding.weight\n",
    "utils.hist_eigenvalues(eigenvalues_dict, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not including unembedding.weight\n",
    "utils.hist_eigenvalues(eigenvalues_dict, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.001,0.003,0.01,0.03]\n",
    "\n",
    "utils.plot_eigenvalues_in_range(eigenvalues_dict,ranges = [()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "inputs = batch.float()\n",
    "targets = batch.float()\n",
    "output = model.float()(inputs)\n",
    "loss = loss_func(output, targets)\n",
    "\n",
    "# Compute the gradient of the loss with respect to the model parameters\n",
    "grad_params = grad(loss, model.parameters(), create_graph=True)\n",
    "print([g.shape for g in grad_params])\n",
    "grad_params = torch.concat(tuple([g.reshape(-1) for g in grad_params]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_only_model_history = {epoch: copy.deepcopy(model) for epoch, model in model_history}\n",
    "model.unembedding.weight[torch.norm(model.embedding.weight.data,dim=0) > 0.05] = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k,v in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "W_E = torch.zeros((n,3))\n",
    "W_U = torch.zeros((n,3))\n",
    "r = np.sqrt(1/(1-np.cos(2*np.pi/n)))\n",
    "b = 1-r**2\n",
    "for i,theta in enumerate(np.arange(n) * 2*np.pi/n):\n",
    "    W_E[i] = torch.tensor([r*np.cos(theta), r*np.sin(theta), np.sqrt(-b)])\n",
    "    W_U[i] = torch.tensor([r*np.cos(theta), r*np.sin(theta), -np.sqrt(-b)])\n",
    "utils.plot_weights_interactive({'Embed': [W_E.T], 'Unembed': [W_U.T]})\n",
    "utils.visualize_matrices_with_slider([F.relu(W_U @ W_E.T)],1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
