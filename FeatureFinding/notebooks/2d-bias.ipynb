{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import plotly.graph_objs as goa\n",
    "import matplotlib as mpl\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import copy\n",
    "from itertools import combinations\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from fancy_einsum import einsum\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n",
    "from typing import Dict, List, Optional, Union, Any, Callable\n",
    "from torch.autograd import grad\n",
    "import sys\n",
    "sys.path.append(\"/Users/jakemendel/Desktop/Code/FeatureFinding\") \n",
    "from FeatureFinding import utils, datasets, models\n",
    "mpl.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['figure.figsize'] = (15,10)\n",
    "fontsize = 20\n",
    "mpl.rcParams['font.size'] = fontsize\n",
    "mpl.rcParams['xtick.labelsize'] = fontsize\n",
    "mpl.rcParams['ytick.labelsize'] = fontsize\n",
    "mpl.rcParams['legend.fontsize'] = fontsize\n",
    "mpl.rcParams['axes.titlesize'] = fontsize\n",
    "mpl.rcParams['axes.labelsize'] = fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 12\n",
    "utils.set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 40\n",
    "k = 1\n",
    "n = 2\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = F.relu\n",
    "tied = False\n",
    "final_bias = True\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "\n",
    "\n",
    "epochs = 250000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 5\n",
    "initial_lr = 0.001\n",
    "warmup_frac = 0.05\n",
    "final_lr = 2\n",
    "decay_factor=(final_lr/max_lr)**(1/(epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "model = utils.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[1000:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_relu = [model(torch.eye(f)).cpu().detach().numpy() for model in model_history.values()]\n",
    "post_softmax = [model(torch.eye(f)).softmax(dim=1).cpu().detach().numpy() for model in model_history.values()]\n",
    "pre_relu = []\n",
    "for model in model_history.values():\n",
    "    out, activations = model(torch.eye(f), hooked=True)\n",
    "    pre_relu.append(activations['unembed_pre'].cpu().detach().numpy())\n",
    "if not MSE:\n",
    "    utils.visualize_matrices_with_slider(post_softmax, store_rate, const_colorbar=True)\n",
    "utils.visualize_matrices_with_slider([p for p in post_relu], store_rate, const_colorbar=True, plot_size = 800)\n",
    "utils.visualize_matrices_with_slider(pre_relu, store_rate, const_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 12\n",
    "utils.set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 40\n",
    "k = 1\n",
    "n = 2\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = F.relu\n",
    "tied = False\n",
    "final_bias = True\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "\n",
    "\n",
    "epochs = 200000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 5\n",
    "initial_lr = 0.001\n",
    "warmup_frac = 0.05\n",
    "final_lr = 2\n",
    "decay_factor=(final_lr/max_lr)**(1/(250000 * (1-warmup_frac)))\n",
    "warmup_steps = int(250000 * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//500\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "model = utils.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the no\n",
    "class ControllableNet(utils.Net):\n",
    "    def __init__(self, net: utils.Net, controlled_rows):\n",
    "        super().__init__(self, net.input_dim, net.hidden_dim, tied = False, final_bias = True, hidden_bias = False, nonlinearity = F.relu, unit_weights=False, learnable_scale_factor = False, standard_magnitude = False, initial_scale_factor = 1.0, initial_embed = None, initial_bias = None)\n",
    "\n",
    "        # Define the input layer (embedding)\n",
    "        self.controlled_mask = torch.zeros(len(net.embedding.weight.data), dtype=torch.bool)\n",
    "        self.controlled_mask[controlled_rows] = True\n",
    "        \n",
    "        self.embedding_free = nn.Linear(self.input_dim - len(controlled_rows), self.hidden_dim, bias=False)\n",
    "        self.embedding_free.weight.data = net.embedding.weight.data[~self.controlled_mask]\n",
    "\n",
    "        self.embedding_controlled = nn.Linear(len(controlled_rows), self.hidden_dim, bias=False)\n",
    "        self.embedding_controlled.weight.data = net.embedding.weight.data[self.controlled_mask]\n",
    "        for param in model.embedding_controlled.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.unembedding_free = nn.Linear(self.hidden_dim, self.input_dim - len(controlled_rows), bias=True)\n",
    "        self.unembedding_free.weight.data = net.unembedding.weight.data[:,~self.controlled_mask]\n",
    "        self.unembedding_free.bias.data = net.unembedding.bias.data[~self.controlled_mask]\n",
    "\n",
    "        self.unembedding_controlled = nn.Linear(self.hidden_dim, len(controlled_rows), bias=True)\n",
    "        self.unembedding_controlled.weight.data = net.unembedding.weight.data[:,self.controlled_mask]\n",
    "        self.unembedding_controlled.bias.data = net.unembedding.bias.data[~self.controlled_mask]\n",
    "\n",
    "        for param in model.unembedding_controlled.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def embedding(self,x):\n",
    "        matrix = torch.zeros(self.input_dim, self.hidden_dim)\n",
    "        matrix[self.controlled_mask] = self.embedding_controlled.weight\n",
    "        matrix[~self.controlled_mask] = self.embedding_free.weight\n",
    "        return matrix@x\n",
    "\n",
    "    def unembedding(self,x):\n",
    "        matrix = torch.zeros(self.hidden_dim, self.input_dim)\n",
    "        matrix[self.controlled_mask] = self.unembedding_controlled.weight\n",
    "        matrix[~self.controlled_mask] = self.unembedding_free.weight\n",
    "        vector = torch.zeros(self.input_dim)\n",
    "        vector[self.controlled_mask] = self.unembedding_controlled.bias\n",
    "        vector[~self.controlled_mask] = self.unembedding_free.bias\n",
    "        return matrix@x + vector\n",
    "\n",
    "\n",
    "    def forward(self, x, hooked = False):\n",
    "        if self.tied:\n",
    "            self.unembedding.weight.data = self.embedding.weight.data.transpose(0, 1)\n",
    "        if hooked:\n",
    "            activations = {}\n",
    "            activations['res_pre'] = self.embedding(x)\n",
    "            activations['unembed_pre'] = self.unembedding(activations['res_pre'])\n",
    "            activations['output'] = self.scale_factor * self.nonlinearity(activations['unembed_pre'])\n",
    "            return activations['output'], activations\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = self.unembedding(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            return self.scale_factor * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your chosen seed\n",
    "chosen_seed = 12\n",
    "utils.set_seed(chosen_seed)\n",
    "\n",
    "#Checking for errors\n",
    "lr_print_rate = 0\n",
    "\n",
    "\n",
    "# Configure the hyperparameters\n",
    "f = 40\n",
    "k = 1\n",
    "n = 2\n",
    "MSE = True #else Crossentropy\n",
    "nonlinearity = utils.relu_plusone\n",
    "tied = True\n",
    "final_bias = True\n",
    "hidden_bias = False\n",
    "unit_weights = False\n",
    "learnable_scale_factor = False\n",
    "initial_scale_factor = 1# (1/(1-np.cos(2*np.pi/f)))**0.5\n",
    "standard_magnitude = False\n",
    "initial_embed = None\n",
    "initial_bias = None\n",
    "\n",
    "\n",
    "epochs = 40000\n",
    "total_epochs = 300000\n",
    "logging_loss = True\n",
    "\n",
    "#Scheduler params\n",
    "max_lr = 5\n",
    "initial_lr = 0.001\n",
    "warmup_frac = 0.05\n",
    "final_lr = 2\n",
    "decay_factor=(final_lr/max_lr)**(1/(total_epochs * (1-warmup_frac)))\n",
    "warmup_steps = int(total_epochs * warmup_frac)\n",
    "\n",
    "\n",
    "store_rate = epochs//100\n",
    "plot_rate=0 #epochs/5\n",
    "\n",
    "\n",
    "# Instantiate synthetic dataset\n",
    "dataset = utils.SyntheticKHot(f,k)\n",
    "batch_size = len(dataset) #Full batch gradient descent\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle = True, num_workers=0)\n",
    "\n",
    "#Define the Loss function\n",
    "criterion = nn.MSELoss() if MSE else nn.CrossEntropyLoss() \n",
    "\n",
    "# Instantiate the model\n",
    "# initial_embed = torch.tensor(np.array([1/(1-np.cos(2*np.pi/f))**0.5*np.array([np.cos(2*np.pi*i/f),np.sin(2*np.pi*i/f)]) for i in range(f)]),dtype=torch.float32).T * 0.5\n",
    "# initial_bias = -torch.ones(f)*(1/(1-np.cos(2*np.pi/f))- 1)*0.25\n",
    "model = utils.Net(f, n,\n",
    "            tied = tied,\n",
    "            final_bias = final_bias,\n",
    "            hidden_bias = hidden_bias,\n",
    "            nonlinearity=nonlinearity,\n",
    "            unit_weights=unit_weights,\n",
    "            learnable_scale_factor=learnable_scale_factor,\n",
    "            standard_magnitude=standard_magnitude,\n",
    "            initial_scale_factor = initial_scale_factor,\n",
    "            initial_embed = initial_embed,\n",
    "            initial_bias = initial_bias)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "\n",
    "#Define a learning rate schedule\n",
    "scheduler = utils.CustomScheduler(optimizer, warmup_steps, max_lr, decay_factor)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "losses, weights_history, model_history = utils.train(model, loader, criterion, optimizer, epochs, logging_loss, plot_rate, store_rate, scheduler, lr_print_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[1000:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_weights_interactive(weights_history, store_rate=store_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_relu = [model(torch.eye(f)).cpu().detach().numpy() for model in model_history.values()]\n",
    "post_softmax = [model(torch.eye(f)).softmax(dim=1).cpu().detach().numpy() for model in model_history.values()]\n",
    "pre_relu = []\n",
    "for model in model_history.values():\n",
    "    out, activations = model(torch.eye(f), hooked=True)\n",
    "    pre_relu.append(activations['unembed_pre'].cpu().detach().numpy())\n",
    "if not MSE:\n",
    "    utils.visualize_matrices_with_slider(post_softmax, store_rate, const_colorbar=True)\n",
    "utils.visualize_matrices_with_slider([p for p in post_relu], store_rate, const_colorbar=True, plot_size = 800)\n",
    "utils.visualize_matrices_with_slider(pre_relu, store_rate, const_colorbar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
